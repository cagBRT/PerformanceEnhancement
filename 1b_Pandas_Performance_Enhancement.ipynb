{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOxSoUPVzoaZqezoiORThI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PerformanceEnhancement/blob/main/1b_Pandas_Performance_Enhancement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Vectorization?<br>\n",
        "\n",
        "Vectorization is the process of applying operations to entire arrays or Series of data, instead of iterating through each element individually.\n",
        "\n",
        "In Pandas, this means that you can perform operations on entire columns or Series without writing explicit loops. This highly efficient approach leverages optimized libraries under the hood, making your code faster and more concise."
      ],
      "metadata": {
        "id": "DOIwkkBSvGTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When to use for loops<br>\n",
        "\n",
        "While for-loop syntax in Python is flexible and provides wonderful utility, each iteration over an element is essentially a single step in the route through all elements of the container object. This step-through processing is useful when the order of operation matters (e.g., returning the first item in a list that meets a certain condition)."
      ],
      "metadata": {
        "id": "gr4LER84ykkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Vectorization is faster\n",
        "\n",
        "**Vectorized processing may be applied when the order of processing does not matter.**\n",
        "\n",
        "The built-in methods in NumPy and Pandas are built with C, which allows for vectorization. Vectorization almost always works faster as execution time is either constant, or grows at a much slower rate with a larger number of elements."
      ],
      "metadata": {
        "id": "_yMsmy-Wyad2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parallel Processing**<br>\n",
        "In NumPy and Pandas, separate segments of arrays are processed amongst all of the processing cores of your computer.\n",
        "\n",
        "NumPy and Pandas operate on their arrays and series in parallel, with a segment of each array being worked on by a different core of your computer’s processor."
      ],
      "metadata": {
        "id": "fsJen9yKy3ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Like-Datatypes**\n",
        "\n",
        "NumPy arrays are set to a single datatype. <br>\n",
        "\n",
        "Likewise with series in Pandas — each column will be of type int, float, str, or datetime.<br>\n",
        "\n",
        "This allows for optimization of data processing, as the contents of these containers are certain to be able to be manipulated in like-manner.<br>\n",
        "\n",
        "**This is not the case with Python’s built-in container data-types, such as lists, sets, and dictionaries.** These types allow you to store a variety of types within them at the same time. A list may contain strings, ints, floats, other lists, etc."
      ],
      "metadata": {
        "id": "_9NurzbP4n4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Locality**<br>\n",
        "NumPy takes your array matrix and stores it in one area of your memory. Contents being local to each other allow them to be operated on faster.<br>\n",
        "\n",
        "In contrast, Python lists may have its contents stored distant from each other within your memory."
      ],
      "metadata": {
        "id": "j-86IPT244l-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Mechanism Behind Vectorization — SISD vs SIMD\n",
        "\n",
        "Modern computer processors contain components that have particular computer architecture classifications that are relevant to understanding vectorization:\n",
        "\n",
        "**SISD — Single Instruction, Single Data**<br>\n",
        "This is the structure for how Python for-loops are processed—\n",
        "One instruction, per one data element, per one moment in time, in order to produce one result.\n",
        "\n",
        "The advantage is that it is flexible — you may implement any operation on your data. <br>\n",
        "The drawback is that it is not optimum for processing large amounts of data.\n",
        "\n",
        "\n",
        "\n",
        "**SIMD—Single Instruction, Multiple Data**<br>\n",
        "This is the structure for how NumPy and Pandas vectorizations are processed—One instruction per any number of data elements per one moment in time, in order to produce multiple results.\n",
        "\n",
        "Contemporary CPUs have a component to process SIMD operations in each of its cores, allowing for parallel processing."
      ],
      "metadata": {
        "id": "tb5szgLK5RHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorized Panda Series and Arrays"
      ],
      "metadata": {
        "id": "qYv4uBazgw7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we compare using apply and series map to datasets over 100 million rows. <br>\n",
        "We then compare these performance values to using vectorized series and arrays."
      ],
      "metadata": {
        "id": "DdjlQP12gbqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gIN--1xQzsRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a dataset of 100 million rows**"
      ],
      "metadata": {
        "id": "Chjo_n5ibcNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_6FgM5szVC9"
      },
      "outputs": [],
      "source": [
        "np.random.seed(3)\n",
        "random_numbers_1=np.random.randint(10e1, size=100000000)\n",
        "random_numbers_2=np.random.randint(10e2, size=100000000)\n",
        "random_numbers_3=np.random.randint(10e3, size=100000000)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'random_numbers_1':random_numbers_1,\n",
        "    'random_numbers_2':random_numbers_2,\n",
        "    'random_numbers_3':random_numbers_3,\n",
        "})\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrame Iterrows:\n",
        "**Iterrows() allows you to iterate through a pandas DataFrame row by row and it’s usually an approach to be avoided.** As in this case, we couldn’t even finish the code within the time limit we set."
      ],
      "metadata": {
        "id": "LnHa-MGez5Cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will take a very long time to run, **don't use iterrows for large datasets**"
      ],
      "metadata": {
        "id": "By9aOEur6QCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a second dataset of 100 million rows, call it df_small.<br>\n",
        "In the assignment we will modify the size of this dataset."
      ],
      "metadata": {
        "id": "1VgY_r5Tg66F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(43)\n",
        "random_numbers_1=np.random.randint(10e1, size=10000000)\n",
        "random_numbers_2=np.random.randint(10e2, size=10000000)\n",
        "random_numbers_3=np.random.randint(10e3, size=10000000)\n",
        "\n",
        "df_small = pd.DataFrame({\n",
        "    'random_numbers_1':random_numbers_1,\n",
        "    'random_numbers_2':random_numbers_2,\n",
        "    'random_numbers_3':random_numbers_3,\n",
        "})\n",
        "df_small"
      ],
      "metadata": {
        "id": "n-6DQfy3Y4I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a large dataset, let's compare using different pandas functions to find the fastest one"
      ],
      "metadata": {
        "id": "9Zrnf8TBhRhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterrows"
      ],
      "metadata": {
        "id": "HXZPCtFDhLX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scalar_iterrows(pandas_df:pd.DataFrame,\n",
        "                             mean_ps:pd.Series,\n",
        "                             std_series:pd.Series,\n",
        "                             )->pd.DataFrame:\n",
        "  \"\"\"Iterate through the rows of the Pandas DataFrame and do the standard scaler calculation row by row\"\"\"\n",
        "\n",
        "  for index, row in pandas_df.iterrows():\n",
        "    pandas_df['scaled_random_numbers_2']=\\\n",
        "    (row[r'random_numbers_2']-mean_ps)/std_series\n",
        "  return pandas_df\n",
        "\n",
        "mean_ps=np.mean(df_small['random_numbers_2'])\n",
        "std_ps=np.std(df['random_numbers_2'])\n",
        "\n",
        "df_small=standard_scalar_iterrows(df_small,\n",
        "                                 mean_ps,\n",
        "                                 std_ps)"
      ],
      "metadata": {
        "id": "C6I5hrTPz5gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### apply to pd.Series"
      ],
      "metadata": {
        "id": "NtlJ8YaEKCum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Times out, don't use this method on large datasets**"
      ],
      "metadata": {
        "id": "4EU8zzjjRS0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scalar_apply(\n",
        "    pandas_ps:pd.Series,\n",
        "    mean_ps:float,\n",
        "    std_ps:float,\n",
        "    ) -> pd.Series:\n",
        "    \"\"\"Use pd.Series.apply() functiom to map through a Panda Series\n",
        "    \"\"\"\n",
        "    scaled_pandas_series=(pandas_ps-mean_ps)/std_pandas_series\n",
        "    return scaled_pandas_series\n",
        "\n",
        "mean_ps=np.mean(df_small['random_numbers_2'])\n",
        "std_pandas_series=np.std(df_small['random_numbers_2'])\n",
        "df_small['scaled_random_numbers_2']=df_small['random_numbers_2'].apply(standard_scalar_apply,\n",
        "                                                                     args=(mean_ps,std_pandas_series))"
      ],
      "metadata": {
        "id": "RCvto1A-KAWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Series Map<br>\n",
        "\n",
        "Choose to map the function over each element within the Pandas Series. This is somewhat faster than Series Apply, but still relatively slow."
      ],
      "metadata": {
        "id": "sL4RAIkwRXlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scalar_map(\n",
        "    pandas_element: int,\n",
        "    mean_pandas_series: float,\n",
        "    std_pandas_series: float\n",
        "    )-> float:\n",
        "    \"\"\"Use pd.Series.map() to map through the elements in Pandas Series\n",
        "    \"\"\"\n",
        "    scaled_pandas_element=(pandas_element-mean_pandas_series)/ std_pandas_series\n",
        "    return scaled_pandas_element\n",
        "\n",
        "mean_pandas_series=np.mean(df['random_numbers_2'])\n",
        "std_pandas_series=np.std(df['random_numbers_2'])\n",
        "df_small['scaled_random_numbers_2'] = df_small['random_numbers_2'].map(lambda x:\n",
        "                                                           standard_scalar_map(x,\n",
        "                                                                               mean_pandas_series=mean_pandas_series,\n",
        "                                                                               std_pandas_series=std_pandas_series\n",
        "                                                                               ))"
      ],
      "metadata": {
        "id": "uZPkDhjsRXyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1**<br>\n",
        "1. Change the df_small dataset to fewer rows.\n",
        "2. What size df_small can be used on the the three previous functions so that it does not time out?"
      ],
      "metadata": {
        "id": "VFiEqptwcC6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RjjB0sumhy50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen that using iterrows, apply, and map on large datasets will lead to the notebook crashing or taking a very long time. <br>\n",
        "\n",
        "Now let's look at what we can use for large datasets"
      ],
      "metadata": {
        "id": "_CvKd0X6hhGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vectorized** Series<br>\n",
        "\n",
        "The definition given by the official Numpy documentation, vectorization is defined as being “able to delegate the task of performing mathematical operations on the array’s contents to optimized, compiled C code.” Instead of looping through rows, columns or elements, this allows us to apply one set of instructions on multiple elements at the same time.\n",
        "\n"
      ],
      "metadata": {
        "id": "zESZrOXdcXy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the built-in vectorization operation from pandas Series with NumPy. Many data operations can and should be vectorized. Even if you don’t have the built-in vectorization operations from pandas Series as custom functions can get complex, you can probably still find many vectorized operations available in Numpy."
      ],
      "metadata": {
        "id": "mOtS12sCcqgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scalar_vectorized_series(\n",
        "    pandas_series: pd.Series\n",
        "  )->pd.Series:\n",
        "  \"\"\"Vectorized operation across Pandas Series\n",
        "  \"\"\"\n",
        "  scaled_series=(pandas_series-np.mean(pandas_series))/np.std(pandas_series)\n",
        "  return scaled_series\n",
        "\n",
        "df['scaled_random_numbers_2']=standard_scalar_vectorized_series(df['random_numbers_2'])"
      ],
      "metadata": {
        "id": "LvNF57ArcX9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorized array<br>\n",
        "\n",
        "By using the NumPy array directly (you can convert Pandas Series to NumPy arrays by calling the .values attribute), you can speed up things even further from the vectorized Series."
      ],
      "metadata": {
        "id": "DcX0RfBgdvU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scaler_vectorized_array(\n",
        "    numpy_array: np.array\n",
        "    )->np.array:\n",
        "    \"\"\"Vectorized operation across numpy arrays\n",
        "    \"\"\"\n",
        "    scaled_array=(numpy_array - np.mean(numpy_array))/np.std(numpy_array)\n",
        "    return scaled_array\n",
        "\n",
        "df['scaled_random_numbers_2']=standard_scaler_vectorized_array(df['random_numbers_2'].values)"
      ],
      "metadata": {
        "id": "OfjXgGGpd21a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2:** <br>\n",
        "How large of a dataset can be used with the vectorized series and arrays before it times out?"
      ],
      "metadata": {
        "id": "3PhaS5lggM5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HxS-K7aPiEOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For 100 million rows of data:**<br>\n",
        "\n",
        "Iterrows: timed out<br>\n",
        "Series Apply: timed out<br>\n",
        "Series Map: timed out<br>\n",
        "Vectorized Series: 1.39 s CPU time<br>\n",
        "Vectorized Arrays: 1.07 s CPU time"
      ],
      "metadata": {
        "id": "lEq7korffYkx"
      }
    }
  ]
}