{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPrQe0Ni+Z8tnyxV4pDZtGE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PerformanceEnhancement/blob/main/pandas_performance_enhancement_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorized Panda Series and Arrays"
      ],
      "metadata": {
        "id": "qYv4uBazgw7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we compare using apply and series map to datasets over 100 million rows. <br>\n",
        "We then compare these performance values to using vectorized series and arrays."
      ],
      "metadata": {
        "id": "DdjlQP12gbqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gIN--1xQzsRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a dataset of 100 million rows**"
      ],
      "metadata": {
        "id": "Chjo_n5ibcNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_6FgM5szVC9"
      },
      "outputs": [],
      "source": [
        "np.random.seed(3)\n",
        "random_numbers_1=np.random.randint(10e1, size=100000000)\n",
        "random_numbers_2=np.random.randint(10e2, size=100000000)\n",
        "random_numbers_3=np.random.randint(10e3, size=100000000)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'random_numbers_1':random_numbers_1,\n",
        "    'random_numbers_2':random_numbers_2,\n",
        "    'random_numbers_3':random_numbers_3,\n",
        "})\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrame Iterrows:\n",
        "**Iterrows() allows you to iterate through a pandas DataFrame row by row and it’s usually an approach to be avoided.** As in this case, we couldn’t even finish the code within the time limit we set."
      ],
      "metadata": {
        "id": "LnHa-MGez5Cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will take a very long time to run, **don't use iterrows for large datasets**"
      ],
      "metadata": {
        "id": "By9aOEur6QCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a second dataset of 100 million rows, call it df_small.<br>\n",
        "In the assignment we will modify the size of this dataset."
      ],
      "metadata": {
        "id": "1VgY_r5Tg66F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(43)\n",
        "random_numbers_1=np.random.randint(10e1, size=10000000)\n",
        "random_numbers_2=np.random.randint(10e2, size=10000000)\n",
        "random_numbers_3=np.random.randint(10e3, size=10000000)\n",
        "\n",
        "df_small = pd.DataFrame({\n",
        "    'random_numbers_1':random_numbers_1,\n",
        "    'random_numbers_2':random_numbers_2,\n",
        "    'random_numbers_3':random_numbers_3,\n",
        "})\n",
        "df_small"
      ],
      "metadata": {
        "id": "n-6DQfy3Y4I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a large dataset, let's compare using different pandas functions to find the fastest one"
      ],
      "metadata": {
        "id": "9Zrnf8TBhRhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterrows"
      ],
      "metadata": {
        "id": "HXZPCtFDhLX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scalar_iterrows(pandas_df:pd.DataFrame,\n",
        "                             mean_ps:pd.Series,\n",
        "                             std_series:pd.Series,\n",
        "                             )->pd.DataFrame:\n",
        "  \"\"\"Iterate through the rows of the Pandas DataFrame and do the standard scaler calculation row by row\"\"\"\n",
        "\n",
        "  for index, row in pandas_df.iterrows():\n",
        "    pandas_df['scaled_random_numbers_2']=\\\n",
        "    (row[r'random_numbers_2']-mean_ps)/std_series\n",
        "  return pandas_df\n",
        "\n",
        "mean_ps=np.mean(df_small['random_numbers_2'])\n",
        "std_ps=np.std(df['random_numbers_2'])\n",
        "\n",
        "df_small=standard_scalar_iterrows(df_small,\n",
        "                                 mean_ps,\n",
        "                                 std_ps)"
      ],
      "metadata": {
        "id": "C6I5hrTPz5gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### apply to pd.Series"
      ],
      "metadata": {
        "id": "NtlJ8YaEKCum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Times out, don't use this method on large datasets**"
      ],
      "metadata": {
        "id": "4EU8zzjjRS0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scalar_apply(\n",
        "    pandas_ps:pd.Series,\n",
        "    mean_ps:float,\n",
        "    std_ps:float,\n",
        "    ) -> pd.Series:\n",
        "    \"\"\"Use pd.Series.apply() functiom to map through a Panda Series\n",
        "    \"\"\"\n",
        "    scaled_pandas_series=(pandas_ps-mean_ps)/std_pandas_series\n",
        "    return scaled_pandas_series\n",
        "\n",
        "mean_ps=np.mean(df_small['random_numbers_2'])\n",
        "std_pandas_series=np.std(df_small['random_numbers_2'])\n",
        "df_small['scaled_random_numbers_2']=df_small['random_numbers_2'].apply(standard_scalar_apply,\n",
        "                                                                     args=(mean_ps,std_pandas_series))"
      ],
      "metadata": {
        "id": "RCvto1A-KAWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Series Map<br>\n",
        "\n",
        "Choose to map the function over each element within the Pandas Series. This is somewhat faster than Series Apply, but still relatively slow."
      ],
      "metadata": {
        "id": "sL4RAIkwRXlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scalar_map(\n",
        "    pandas_element: int,\n",
        "    mean_pandas_series: float,\n",
        "    std_pandas_series: float\n",
        "    )-> float:\n",
        "    \"\"\"Use pd.Series.map() to map through the elements in Pandas Series\n",
        "    \"\"\"\n",
        "    scaled_pandas_element=(pandas_element-mean_pandas_series)/ std_pandas_series\n",
        "    return scaled_pandas_element\n",
        "\n",
        "mean_pandas_series=np.mean(df['random_numbers_2'])\n",
        "std_pandas_series=np.std(df['random_numbers_2'])\n",
        "df_small['scaled_random_numbers_2'] = df_small['random_numbers_2'].map(lambda x:\n",
        "                                                           standard_scalar_map(x,\n",
        "                                                                               mean_pandas_series=mean_pandas_series,\n",
        "                                                                               std_pandas_series=std_pandas_series\n",
        "                                                                               ))"
      ],
      "metadata": {
        "id": "uZPkDhjsRXyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1**<br>\n",
        "1. Change the df_small dataset to fewer rows.\n",
        "2. What size df_small can be used on the the three previous functions so that it does not time out?"
      ],
      "metadata": {
        "id": "VFiEqptwcC6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RjjB0sumhy50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen that using iterrows, apply, and map on large datasets will lead to the notebook crashing or taking a very long time. <br>\n",
        "\n",
        "Now let's look at what we can use for large datasets"
      ],
      "metadata": {
        "id": "_CvKd0X6hhGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vectorized** Series<br>\n",
        "\n",
        "The definition given by the official Numpy documentation, vectorization is defined as being “able to delegate the task of performing mathematical operations on the array’s contents to optimized, compiled C code.” Instead of looping through rows, columns or elements, this allows us to apply one set of instructions on multiple elements at the same time.\n",
        "\n"
      ],
      "metadata": {
        "id": "zESZrOXdcXy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the built-in vectorization operation from pandas Series with NumPy. Many data operations can and should be vectorized. Even if you don’t have the built-in vectorization operations from pandas Series as custom functions can get complex, you can probably still find many vectorized operations available in Numpy."
      ],
      "metadata": {
        "id": "mOtS12sCcqgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scalar_vectorized_series(\n",
        "    pandas_series: pd.Series\n",
        "  )->pd.Series:\n",
        "  \"\"\"Vectorized operation across Pandas Series\n",
        "  \"\"\"\n",
        "  scaled_series=(pandas_series-np.mean(pandas_series))/np.std(pandas_series)\n",
        "  return scaled_series\n",
        "\n",
        "df['scaled_random_numbers_2']=standard_scalar_vectorized_series(df['random_numbers_2'])"
      ],
      "metadata": {
        "id": "LvNF57ArcX9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorized array<br>\n",
        "\n",
        "By using the NumPy array directly (you can convert Pandas Series to NumPy arrays by calling the .values attribute), you can speed up things even further from the vectorized Series."
      ],
      "metadata": {
        "id": "DcX0RfBgdvU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def standard_scaler_vectorized_array(\n",
        "    numpy_array: np.array\n",
        "    )->np.array:\n",
        "    \"\"\"Vectorized operation across numpy arrays\n",
        "    \"\"\"\n",
        "    scaled_array=(numpy_array - np.mean(numpy_array))/np.std(numpy_array)\n",
        "    return scaled_array\n",
        "\n",
        "df['scaled_random_numbers_2']=standard_scaler_vectorized_array(df['random_numbers_2'].values)"
      ],
      "metadata": {
        "id": "OfjXgGGpd21a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2:** <br>\n",
        "How large of a dataset can be used with the vectorized series and arrays before it times out?"
      ],
      "metadata": {
        "id": "3PhaS5lggM5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HxS-K7aPiEOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For 100 million rows of data:**<br>\n",
        "\n",
        "Iterrows: timed out<br>\n",
        "Series Apply: timed out<br>\n",
        "Series Map: timed out<br>\n",
        "Vectorized Series: 1.39 s CPU time<br>\n",
        "Vectorized Arrays: 1.07 s CPU time"
      ],
      "metadata": {
        "id": "lEq7korffYkx"
      }
    }
  ]
}